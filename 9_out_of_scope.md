# out_of_scope
この文書の中では対象としない範囲
## 触覚
- ロボットの分野で、触覚は重要な感覚になっている。
- そのため、触覚情報を用いることがタスクによっては、不可欠になっている。
- 例：イチゴの収穫

触覚自体は、visionの分野の対象外となる。しかし、ロボットなどの実空間で動作を必要とするときには、触覚の問題を避けて通れない。
適切な触覚を導入することで、vision分野のタスクを軽減できる。
また、触覚の理解の分野に、画像認識と同様な手法が導入されつつある。

### 視触覚センサ
- [視触覚センサ Finger Vision](https://www.fingervision.jp/service-product)
透明な弾性体の変形を表面にあるドッドマーク位置の変位によりとらえて、分布を計測することができる。
透明な弾性体越しに見える画像を元にした視覚的な判断も可能である。

### 近接覚センサ
[近接覚センサ](https://www.thinker-robotics.co.jp/product/sensor-tk-01.html)
近接覚センサーは、赤外線と独自のエッジAIの組み合わせにより、
対象物との「距離および姿勢」を非接触かつ高分解能でリアルタイムに計測できるセンサーです。
従来は困難とされていた透明物や鏡面物の計測も可能で、
これまで導入が諦められていたさまざまな現場でもロボットハンドを活用できるようになります。


[触覚・近接覚センサ解説 V3](https://www.docswell.com/s/m_shimojo/Z8RPP5-2022-04-16-134500)

## end-to-endの学習
最近、大規模言語モデルの進展は、ロボットの動作の分野にも及んできている。

全てのセンサの入力を同時に与えて、ロボットの動作をend-to-end で学習させようとするアプローチである。
大規模言語モデルが静止画・動画・マルチモーダルな時系列データに対して利用可能になったことにより、
開発が進んでいる分野である。

この文書の範囲では、３Dカメラの評価、個々のアルゴリズムインタフェースの標準化などを意図したものであり、
end-to-end の機械学習については取り扱わない。

#### ロボットアーム・エンドエフェクタの情報の取扱い
- 
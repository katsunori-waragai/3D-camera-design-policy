# タスクの再設計
画像認識・画像計測・機械学習は、実現しようとする内容を、どのようなタスクに分解するのかによって、
実現用の容易性が格段に変わってくる。

#### 例：検出タスクの共通化
よい実装があれば、対象物によって違う複数の検出タスクを、同時に実行する必要がなくなります。
###### 場合１：　人物検出器（全身）と顔検出・照合ライブラリ
　人物検出自体には、顔検出をできないとすると、全てのフレームで、顔検出・照合ライブラリを呼び出さなくてはならなくなります。
###### 場合２：頭部検出もできる人物検出器で、頭部の向きも推定できる場合
　頭部検出もできる人物検出器で、頭部の向きも推定して、顔照合可能な顔向きであったときだけ、
　顔照合ライブラリを呼び出す。
参考事例：https://github.com/PINTO0309/PINTO_model_zoo

google colabotory でのノートブック
https://colab.research.google.com/drive/1Klphx3MKgOcz2CGmVOz09gYiUL76IXZM?usp=share_link


###### 場合３：頭部検出もできる人物検出器で、頭部の向きも推定できる場合でかつ、頭部の追跡ができている場合
　追跡に成功している顔を、顔照合する頻度を減らすことができます。
　3Dカメラによって対応付けられている人物検出は、３Ｄ空間の座標として追跡をしているので、追跡の間違いが発生しにくくなっています。
　（2Dでの追跡にしても、YOLOシリーズとDeepSORTとの組み合せによって、従来よりも誤追跡の発生がしにくくなっています。）

使用するライブラリによって、必要なリソースが大幅に減ってきます。
　人・頭部・手検出など、利用上の価値が高い検出と属性推定を１つのモデルにまとめてあるのは、極めて有益です。

　　　　　
#### 例：ステレオ計測タスクとoptical flowのタスクの共通化
ステレオ計測タスクでは、１つのフレームでの左画像と右画像からの深度情報の推定（＝点群の座標の推定）を行う。
２つのフレーム間で、対応点の移動量を計算できれば、それはoptical flow そのものである。
自分以外のものが動いていなれば、自らの運動に起因するoptical flow の方向と大きさで、
対象物体の位置を推定できる。

これに、動いている対象物が加わったときに、
自身の動きでは説明がつかないものの動きを検出できる。
駐車場で止まっている自動車と、動いている自動車を、optical flow ベースで区別できているはずである。

LSTM(Long short-term memory) を使っているアプローチはみかけるのだが。

### 例：optical flow ベースの衝突回避の移動
画像認識系のタスクが、次の行動選択のための入力になっていることがある。
その場合、認識系のタスクと行動選択のタスクを１つにまとめて学習・最適化を実施するアプローチがある。
そうすることで、タスク間の連結が簡単になり、階層が少なくなることで遅延時間を低下させる効果も持つ。


Obstacle Detection: Zhao, Y., & Shi, Y. (2016). A novel obstacle detection method based on optical flow. In 2016 International Conference on Control, Automation, Robotics and Vision (ICARCV).

[Optical-Flow-based-Obstacle-Avoidance](https://github.com/zainmehdi/Optical-Flow-based-Obstacle-Avoidance)

#### 注意：問題の定式化としてほんとうに自身の例に対して適切なのかを疑うこと
- 見通しのよい十字型の交差点で起きる事故に対して、optical flow ベースの衝突回避の移動　は無力である。


### 自己視点画像での自分の手と対象物との理解というタスク
近年、ロボットの分野で進んできているタスクが、
自己視点画像での自分の両手と扱おうとしている対象物への理解である。

##### 見えている対象物についてのopen vocabulary（＝限定されない言葉）での理解。
大規模言語モデルでの理解は、対象物に対して起こしうる動作の関係付けがある。
アフォーダンスという定式化がされるようになってきている。
ペットボトルをつかむという動作の選択肢が、ペットボトルという物体がトリガーになって引き起こされるという考え方である。
このとき、「ペットボトルをつかむ」、「ペットボトルのキャップを外す」、「ペットボトルの中身をカップに注ぐ」
などという動作の種類によって、手を当てる位置や、次に引き起こす動作が変わってくる。
この領域の分野は、近年研究が進んでいる分野である。

## 新規のタスクは、新規のデータ・セットを必要とする。
新規のタスクの宣言と、そのタスクの学習と評価の実験が必要になる。
これは、もはや一人の開発者でできることではなくなっている。
この分野の研究を行うためには、所属している部署でチームを組む必要がある。
チームで、その課題に対して取り組むことを決断しないと、新規のタスクに対して勝負を仕掛けることができない。

## 自前データセットの重要性
- 自社の製品・サービスに機械学習タスクを組み込む場合、評価が重要になってくる。
- 大規模言語モデルのように汎用の目的に訓練されている場合でも、 自社の製品・サービスで性能が満たしているのかどうかは、 開発者チームの責任である。

## どういう制約が可能であるか
- 画像認識タスクは、カメラ入力データの品質という問題がつきまとい続ける。
- そのため、性能が確保できるのは、いくつかの制約の範囲となる。
- 実現しようとする作業が明確になっている場合には、品質を確保する条件として、動作条件に制約をつけよう。
- その制約の範囲では、画像認識タスクは品質が安定しやすい。

## 問いかけ：３Dカメラの空間認識では、今見えていない領域をどうすべきか？
### 可能性１：見えている領域しか扱わない
　実装が簡単な手法である。しかし、隠れや未検出が生じたとき、状況の解釈が欠落する。
### 可能性２：見えていない領域でも、統計的に推論する
　人はペットボトルをつかむ時に、見えない反対側の領域を推測している。
　見えている範囲だけにこだわるかぎり、見えていない範囲の位置を想定して、手の開き具合を調節することができない。
　だから、見えていない領域に対しても、統計的な推論を行うことが必要になるだろうと予測する。
### 可能性３：見えていた領域は、点群としての理解を保つ
　階段を昇る際には、足元を見続けながら階段を昇るやり方と、階段の状況を覚えておいて、足元を見ないで昇る方法とがある。
　

## 利用可能になっているタスク
#### 2Dでの検出結果を3Dとして解釈しなおすこと。 3Dとしての物体追跡
例：StereoLabsのZED　SDKにおいては、物体検出の結果を、３Dとして当てはめる機能がある。
　そのことによって、物体検出結果の３D空間での追跡が可能になっている。
　３Dでの追跡の場合、対象物が重なることがないため、人物が前後方向に重なっても、安定して追跡できる。
コード例
https://github.com/stereolabs/zed-sdk/blob/master/object%20detection/custom%20detector/python/pytorch_yolov8/detector.py

```
det = model.predict(img, save=False, imgsz=img_size, conf=conf_thres, iou=iou_thres)[0].cpu().numpy().boxes
detections = detections_to_custom_box(det, image_net)
```

3Dとして解釈し直したあとのデータ型
sl.ObjectData　型
bounding_box　# 直方体としてのbounding boxです。8点の空間座標があります。
dimensions # その座標系と単位での物体のbounding boxの大きさになります。

解説記事：
[ZED SDK でcustom detectorを使う](https://qiita.com/nonbiri15/items/05c9a9cc7066b0ba04cf)

この２Dでの検出結果を３Dとして解釈し直す仕組みは、他の３Dカメラに対しても有効である。

#### Open Vocabulary での物体検出・セグメンテーション
視覚言語モデルの進展は、従来の限られた物体検出から、任意の言葉による物体検出を可能にしている。
それらは、NVIDIA Jetson のような組み込み可能なデバイスにも移植されている。

- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)
- [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)


## 新たな定式化の１例
論文 "Formula-Supervised Visual-Geometric Pre-training" です！
2D画像・3D点群のモダリティを単一Transformerで扱い、識別・検出・領域分割タスク全てで事前学習効果があります。
今後、Multimodal Pre-training手法として拡張できるポテンシャルがあります。
[Formula-Supervised Visual-Geometric Pre-training](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03233.pdf)
[project page](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3233_ECCV_2024_paper.php)

## 過去は別タスクだったものが、同一のタスクになったもの
- 人の各部位の物体検出
  - 人体全体・頭部・眼・鼻・口・手の検出
  - https://github.com/PINTO0309/PINTO_model_zoo/tree/main/459_YOLOv9-Wholebody25
- 物体検出とインスタンスセグメンテーション
  - 例: YoloV9など
